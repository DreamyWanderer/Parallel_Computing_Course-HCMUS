{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGBgThS8q8k3"
      },
      "source": [
        "Họ và Tên: Nguyễn Thế Hoàng\n",
        "\n",
        "MSSV: 20120090"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdrvDrCrnqz"
      },
      "source": [
        "# HW0: Làm quen với CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKXB0wA7yhq9"
      },
      "source": [
        "Với các GPU tương đối mới thì để biên dịch chỉ cần dùng câu lệnh: \\\n",
        "`nvcc tên-file.cu -o tên-file-chạy`\n",
        "\n",
        "Nhưng trên Colab mình thường lấy được GPU khá cũ là Tesla K80 với compute capability (phiên bản phần cứng) là 3.7; để biên dịch đúng với GPU khá cũ này thì bạn cần dùng câu lệnh: \\\n",
        "`nvcc -arch=sm_37 tên-file.cu -o tên-file-chạy` \\\n",
        "Trong đó, 37 chính là compute capability của GPU Tesla K80.\n",
        "\n",
        "Để phòng trường hợp khi làm bài bạn lấy được GPU có compute capability x.x nhưng khi chấm bài Thầy lại lấy được GPU có compute capability khác x.x, dưới đây mình sẽ có đoạn code Python để tự động lấy 2 con số ứng với compute capability của GPU và lưu vào 2 biến `major` và `minor`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCkmnirl2xWF"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "major, minor = cuda.get_current_device().compute_capability\n",
        "print(f'GPU compute capability: {major}.{minor}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq1-pmi72yS6"
      },
      "source": [
        "Một khi đã chạy đoạn code Python ở trên, để biên dịch thì bạn sẽ dùng câu lệnh: \\\n",
        "`nvcc -arch=sm_{major}{minor} tên-file.cu -o tên-file-chạy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkZaH7EE-ocN"
      },
      "source": [
        "Dưới đây, khi làm bài thì bạn có thể tùy ý thêm/xóa cell. Đừng xóa mấy cell có chữ của Thầy là được."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH9lSjFfr3Kw"
      },
      "source": [
        "## Câu 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZNqZuECjNso"
      },
      "source": [
        "### The GPU Specification of Colab\n",
        "\n",
        "Hello World, I made by a person named The Hoang!\n",
        "\n",
        "- GPU card's name: Tesla T4\n",
        "- GPU computation capabilities: 7.5\n",
        "- Maximum number of block dimensions: 3 dimensions\n",
        "- Maximum size of each block dimensions: 1024.x 1024.y 64.z\n",
        "- Maximum number of grid dimensions: 3 dimesions\n",
        "- Maximum size of each grid dimensions: 2147483647.x 65535.y 65535.y\n",
        "- Maximum size of GPU memory: 15835398144 bytes\n",
        "- Size of constant memory: 65536 bytes\n",
        "- Size of shared memory per multiprocessor and in GPU device: 65536 bytes and 2621440 bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlycLWxberDO"
      },
      "source": [
        "## Câu 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE-rY5TesEFe"
      },
      "source": [
        "|     Vector Size     |       Host time       |       Device time (version 1)    |    Device time (version 2)    |\n",
        "| --- | --- | --- | --- |\n",
        "|\t64        \t|\t0.008448  \t|\t0.028832            \t|\t0.017088            |\n",
        "|\t256       \t|\t0.009408  \t|\t0.020544            \t|\t0.018144            |\n",
        "|\t1024      \t|\t0.010912  \t|\t0.021088            \t|\t0.018816            |\n",
        "|\t4096      \t|\t0.025152  \t|\t0.021248            \t|\t0.016992            |\n",
        "|\t16384     \t|\t0.078240  \t|\t0.020896            \t|\t0.017376            |\n",
        "|\t65536     \t|\t0.298720  \t|\t0.023584            \t|\t0.017856            |\n",
        "|\t262144    \t|\t1.152960  \t|\t0.032768            \t|\t0.026464            |\n",
        "|\t1048576   \t|\t6.466848  \t|\t0.077568            \t|\t0.083264            |\n",
        "|\t4194304   \t|\t19.928225 \t|\t0.217216            \t|\t0.268352            |\n",
        "|\t16777216  \t|\t78.469536 \t|\t0.799424            \t|\t1.009728            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG1mOFWW7zlB"
      },
      "source": [
        "### Insight\n",
        "\n",
        "- With the really small array, the host version executes more quickly than both device versions (it may dues to the overhead of workload when controlling the parallel computing). On the other hand, with huge array size, both device versions outperforms the host version in time implementing.\n",
        "- The device version 2 execute quite more quickly than device version 1, and vice versa."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
