{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGBgThS8q8k3"
   },
   "source": [
    "Full name: Nguyễn Thế Hoàng\n",
    "\n",
    "Student ID: 20120090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qdrvDrCrnqz"
   },
   "source": [
    "# HW2: Parallel Execution in CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKXB0wA7yhq9"
   },
   "source": [
    "**To compile your file, you can use this command:** \\\n",
    "`nvcc tên-file.cu -o tên-file-chạy` \\\n",
    "***You can use Vietnamese to anwser the questions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "major, minor = cuda.get_current_device().compute_capability\n",
    "print(f'GPU compute capability: {major}.{minor}')\n",
    "\n",
    "wkDir = './'\n",
    "p1Path = f'{wkDir}HW2_P1.cu'\n",
    "p1Exe = f'{wkDir}P1.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_{major}{minor} --ptxas-options=-v {p1Path} -o {p1Exe}\n",
    "!nvprof {p1Exe}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH9lSjFfr3Kw"
   },
   "source": [
    "## Question 1A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZNqZuECjNso"
   },
   "source": [
    "| Kernel | Execute Time (ms) |\n",
    "| -------- | -------- |\n",
    "| Warmup | 0.230 |\n",
    "| Host | 44.758 |\n",
    "| Kernel 1 | 2.710 |\n",
    "| Kernel 2 | 1.813 |\n",
    "| Kernel 3 | 1.308 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XebMjR45-0Io"
   },
   "source": [
    "### Reasoning\n",
    "\n",
    "- In the Kernel 1: this segment of condition checking\n",
    "\n",
    "    ```if (threadIdx.x % stride == 0)```\n",
    "\n",
    "    only let the thread at the head of the current stride do computation. So this will cause warp divergence. However, the unrolling in this kernel also help speed things a little (each thread block sums two consecutive data block).\n",
    "\n",
    "- In the Kernel 2: the thread is not fixed in the elements it need to compute. The thread can be repositioned in each stride. The number of active threads is reduced by half after each round. The first half of threads in a block is always active (while the second half is inactive). The divergece only happens when the number of active threads in the block in some last rounds smaller than the size of warp ($< 32$).\n",
    "\n",
    "- In the Kernel 3: the working threads are not changed when compared with the Kernel 2. However, this can help improve the global memory load/store patterns (as explained in \"Professional CUDA C Programming\" textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU activities\n",
    "\n",
    "![nvprof of part 1](./Figure/nvprof_part_1.png)\n",
    "\n",
    "#### Comment\n",
    "\n",
    "- The program uses most of the time to copy data from Host to Device memory. On the other hand, the program does not use much time to copy data back to Host memory from the Device. This is only true in this case since the amount of data in Device-to-Host-copying process is really small. So I deduce that in general, the copy data processes between Host and Device memory consomue most of the program execution time.\n",
    "\n",
    "- The time used by each type of kernel is the same as expected: Kernel 1 ran slowest; Kernel 2 ran more quickly; and Kernel 3 is the quickest in execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9wMWgeV--5b"
   },
   "source": [
    "## Question 1B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMsckPIh_Ije"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_{major}{minor} --ptxas-options=-v {p1Path} -o {p1Exe}\n",
    "!nvprof {p1Exe} 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydvO00hC_JMW"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_{major}{minor} --ptxas-options=-v {p1Path} -o {p1Exe}\n",
    "!nvprof {p1Exe}256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoxamhSa_Jjc"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_{major}{minor} --ptxas-options=-v {p1Path} -o {p1Exe}\n",
    "!nvprof {p1Exe} 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MF_Kjjqe_J3F"
   },
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_{major}{minor} --ptxas-options=-v {p1Path} -o {p1Exe}\n",
    "!nvprof {p1Exe} 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9PXsn1C_L4L"
   },
   "source": [
    "Block size | Grid size | Num blocks / SM | Occupancy (%) | Kernel time (ms)\n",
    "--- | --- | --- | --- | --- \n",
    "1024 | 8193 | 1 | 100 | 2.714 \n",
    "512 | 16385 | 2 | 100 | 1.992 \n",
    "256 | 32769 | 4 | 100 | 1.632 \n",
    "128 | 65537 | 8 | 100 | 1.397 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c811YinAqrd"
   },
   "source": [
    "$\\text{Num blocks/SM} = \\dfrac{ \\text{Maximum threads per SM} }{ \\text{Threads per block} }$\n",
    "\n",
    "$\\text{Occupancy} = \\dfrac{ \\text{Number of active warps per SM} }{ \\text{Maximum number of warps per SM} }$\n",
    "\n",
    "Although all kernels have the same ocupancy, the kernel time is decreasing as the block size decreases. Since when the block size decreases, there are more active blocks and warps to provide for each SM. There are also less arithmetic operations in each block for executing, so the execution time for each block decreases. The CPU can reduce the remained sum from each blocks quickly so the final execution time is the less for kernel use the less size of block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Kernel | Execute Time (ms) |\n",
    "| -------- | -------- |\n",
    "| Warmup | 0.074 |\n",
    "| Host | 9522.951 |\n",
    "| Basic (No shared memory) | 7.795 |\n",
    "| Using shared memory | 3.977 ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Figure/nvprof_part_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now even the kernel 1 is slower than the transfer data between Host/Device operation. The Device-to-Host-copying process is same as Host-to-Device-copying process in execution time - this proves the point that I have assumed above in Part 1 since in this case, the output result transfered from Device to Host is quite large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning\n",
    "\n",
    "It is clear that when we use shared memory, the execution time decreases dramatically. Because in each block, there are many threads that use the same rows/columns of A, B matrix for computing. It is a waste to always get them from the global memory. So for each block, we detect the elements from A and B that we need to have for computing of that block (in C) and copy required elements from A/B to shared memory in block for quicker access speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwzjJVcZE2Yc"
   },
   "source": [
    "## Question 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "major, minor = cuda.get_current_device().compute_capability\n",
    "print(f'GPU compute capability: {major}.{minor}')\n",
    "\n",
    "wkDir = './'\n",
    "p2Path = f'{wkDir}HW2_P2.cu'\n",
    "p2Exe = f'{wkDir}P2.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=sm_{major}{minor} {p2Path} -o {p2Exe}\n",
    "!nvprof {p2Exe}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Basic Matrix Multipication**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many floating operations are being performed in your matrix multiply\n",
    "kernel? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2 \\times n$. For each thread, we compute $n$ floating multiplication with respective $A$ and $B$ element and use $n$ floating addition to sum the result for that thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many global memory reads are being performed by your kernel? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2 \\times n$. For each thread, we read $n$ elements from $A$ and $n$ elements from $B$ to compute result for that thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many global memory writes are being performed by your kernel? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use constant memory to keep temporary sum result in the computing process, we only need $1$ global memory write when assign that sum to the suitable position in matrix $C$. If we do not use constant memory, we need $n$ times global memory writes to accumulate result to an element in $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Tiled Matrix Multipication**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many floating operations are being performed in your matrix multiply\n",
    "kernel? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2 \\times n$. Each thread also need $n$ floating multiplication with respective $A$ and $B$ element, and $n$ accumulation operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many global memory reads are being performed by your kernel? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $n \\ mod \\ \\text{TILE\\_STRIDE} = 0$ (there is no threads that positioned outside of valid range of $A$ or $B$): $2 \\times \\lceil \\dfrac{n}{\\text{TILE\\_STRIDE}} \\rceil$. Each thread reads $\\lceil \\dfrac{n}{\\text{TILE\\_STRIDE}} \\rceil$ from $A$ and $\\lceil \\dfrac{n}{\\text{TILE\\_STRIDE}} \\rceil$ from $B$ to copy them to shared memory.\n",
    "\n",
    "- If $n \\ mod \\ \\text{TILE\\_STRIDE} \\neq 0$ (there is threads that positioned outside of valid range of $A$ or $B$):\n",
    "\n",
    "    - If thread is in range of $A$ and $B$: same as above case.\n",
    "\n",
    "    - If thread is outside of range of $A$ and $B$: $2 \\times \\lceil \\dfrac{n}{\\text{TILE\\_STRIDE}} \\rceil - 2$. We minus $2$ because this thread is always positioned at the outside range of both $A$ and $B$. Here we do not read any thing from $A$ and $B$ so we do not conduct global memory reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many global memory writes are being performed by your kernel? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as the basic matrix multiplication case mentioned above."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
